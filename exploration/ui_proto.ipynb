{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import countergen"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Choose your dataset\n",
                "Put your file next to the notebook and give it's filename here:\n",
                "\n",
                "[Box where you can put your filename, with a convincing default dataset]\n",
                "\n",
                "[Check that it exists]\n",
                "\n",
                "[Print a few samples]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from countergen.augmentation import Dataset\n",
                "ds = Dataset.from_jsonl(\"D:\\_Docs\\Programmation\\Python\\Counterfactual-Dataset-Generator\\countergen\\data\\datasets\\doublebind.jsonl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Choose your augmenters\n",
                "\n",
                "[Checkboxes with title and explanation]\n",
                "\n",
                "[Link to somewhere if you want to add your own augmenters]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from countergen.augmentation import SimpleAugmenter\n",
                "augmenters = [SimpleAugmenter.from_default(\"gender\")]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Convert!\n",
                "\n",
                "(Just run this cell)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "aug_ds = ds.augment(augmenters)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Choose models to evaluate\n",
                "\n",
                "[Checkboxs of different models compatible with the dataset you have chosen]\n",
                "\n",
                "[Link to somewhere if you want to evaluate other models]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from countergen.evaluation import get_evaluator_for_generative_model, pt_to_generative_model\n",
                "from countergen.tools.utils import get_device\n",
                "from transformers import GPT2LMHeadModel\n",
                "import torch\n",
                "\n",
                "model_names = [\"distilgpt2\"]\n",
                "device = get_device()\n",
                "model: torch.nn.Module = GPT2LMHeadModel.from_pretrained(model_names[0]).to(device)\n",
                "model_ev = get_evaluator_for_generative_model(pt_to_generative_model(model), \"probability\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4b. Choose your metrics\n",
                "\n",
                "[Checkboxs of different metrics & aggregation methods]\n",
                "\n",
                "[Link to somewhere if you want to add your own]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "aggregator = countergen.evaluation.PerformanceStatsPerCategory()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluate!\n",
                "\n",
                "(Just run this cell)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = countergen.evaluation.evaluate(aug_ds.samples, model_ev, aggregator)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. The results\n",
                "\n",
                "[Beautiful displays]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "aggregator.display({\"distilgpt2\":results})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Edit..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from countergen.editing import ActivationsDataset, get_mlp_modules\n",
                "layers_dict = get_mlp_modules(model, [2,3])\n",
                "layers = list(layers_dict.values())\n",
                "act_ds = ActivationsDataset.from_augmented_samples(aug_ds.samples, model, layers)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from countergen.editing.direction_algos import inlp\n",
                "dirs = inlp(act_ds, n_training_iters=10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from countergen.editing import edit_model, get_edit_configs\n",
                "configs = get_edit_configs(layers_dict, dirs, has_leftover=False)\n",
                "new_model = edit_model(model, configs=configs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_model_ev = get_evaluator_for_generative_model(pt_to_generative_model(new_model), \"probability\")\n",
                "new_results = countergen.evaluation.evaluate(aug_ds.samples, new_model_ev, aggregator)\n",
                "aggregator.display({\"newdistilgpt2\":new_results})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from countergen.editing.direction_algos import rlace\n",
                "dirs = rlace(act_ds, n_dim=2, out_iters=100, num_clfs_in_eval=1, evalaute_every=50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from countergen.editing import edit_model, get_edit_configs\n",
                "configs = get_edit_configs(layers_dict, dirs, has_leftover=False)\n",
                "new_model = edit_model(model, configs=configs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_model_ev = get_evaluator_for_generative_model(pt_to_generative_model(new_model), \"probability\")\n",
                "new_results = countergen.evaluation.evaluate(aug_ds.samples, new_model_ev, aggregator)\n",
                "aggregator.display({\"newdistilgpt2\":new_results})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7.13 ('seven')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.13"
        },
        "vscode": {
            "interpreter": {
                "hash": "c19ce8411f37a8b8579beb4987dd9b292a41194c1fe3735d596a9a084344949b"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
